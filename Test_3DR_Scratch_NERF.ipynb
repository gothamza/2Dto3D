{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Gd1EDkyz_tbd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PodcQObRg7l",
        "outputId": "7e3e5ac3-7960-453e-920a-182c3ebb6be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'zero123plus'...\n",
            "remote: Enumerating objects: 182, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 182 (delta 21), reused 37 (delta 16), pack-reused 132\u001b[K\n",
            "Receiving objects: 100% (182/182), 2.32 MiB | 3.54 MiB/s, done.\n",
            "Resolving deltas: 100% (88/88), done.\n",
            "/content/zero123plus\n",
            "Collecting git+https://github.com/facebookresearch/segment-anything.git (from -r requirements.txt (line 11))\n",
            "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-rbeqee0r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-rbeqee0r\n",
            "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.23.5)\n",
            "Collecting rembg (from -r requirements.txt (line 4))\n",
            "  Downloading rembg-2.0.53-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.8.0.76)\n",
            "Collecting diffusers==0.20.2 (from -r requirements.txt (line 6))\n",
            "  Downloading diffusers-0.20.2.tar.gz (989 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.1/989.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.29.2 (from -r requirements.txt (line 7))\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting streamlit==1.22.0 (from -r requirements.txt (line 8))\n",
            "  Downloading streamlit-1.22.0-py2.py3-none-any.whl (8.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.2.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.19.4)\n",
            "Collecting gradio>=3.50 (from -r requirements.txt (line 12))\n",
            "  Downloading gradio-4.10.0-py3-none-any.whl (16.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire (from -r requirements.txt (line 13))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.2->-r requirements.txt (line 6)) (7.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.2->-r requirements.txt (line 6)) (3.13.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.2->-r requirements.txt (line 6)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.2->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.2->-r requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.20.2->-r requirements.txt (line 6)) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 7)) (6.0.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.2->-r requirements.txt (line 7))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.2->-r requirements.txt (line 7)) (4.66.1)\n",
            "Requirement already satisfied: blinker>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (1.4)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (5.3.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (8.1.7)\n",
            "Requirement already satisfied: pandas<3,>=0.25 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (1.5.3)\n",
            "Requirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (10.0.1)\n",
            "Collecting pympler>=0.9 (from streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (2.8.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (13.7.0)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (8.2.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (5.2)\n",
            "Collecting validators>=0.2 (from streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
            "Collecting gitpython!=3.1.19 (from streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck>=0.1.dev5 (from streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.22.0->-r requirements.txt (line 8)) (6.3.2)\n",
            "Collecting watchdog (from streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg->-r requirements.txt (line 4)) (4.19.2)\n",
            "Collecting onnxruntime (from rembg->-r requirements.txt (line 4))\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg->-r requirements.txt (line 4)) (4.8.1.78)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg->-r requirements.txt (line 4)) (1.8.0)\n",
            "Collecting pymatting (from rembg->-r requirements.txt (line 4))\n",
            "  Downloading PyMatting-1.1.12-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg->-r requirements.txt (line 4)) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg->-r requirements.txt (line 4)) (1.11.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 9)) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<5->-r requirements.txt (line 9)) (0.12.0)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastapi (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.7.3 (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading gradio_client-0.7.3-py3-none-any.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.8/304.8 kB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.50->-r requirements.txt (line 12)) (6.1.1)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.50->-r requirements.txt (line 12)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.50->-r requirements.txt (line 12)) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading orjson-3.9.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic>=2.0 (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.9/381.9 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio>=3.50->-r requirements.txt (line 12)) (0.9.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio-client==0.7.3->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 13)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 13)) (2.4.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19->streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.20.2->-r requirements.txt (line 6)) (3.17.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg->-r requirements.txt (line 4)) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg->-r requirements.txt (line 4)) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg->-r requirements.txt (line 4)) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg->-r requirements.txt (line 4)) (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.50->-r requirements.txt (line 12)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.50->-r requirements.txt (line 12)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.50->-r requirements.txt (line 12)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.50->-r requirements.txt (line 12)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio>=3.50->-r requirements.txt (line 12)) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=0.25->streamlit==1.22.0->-r requirements.txt (line 8)) (2023.3.post1)\n",
            "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
            "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading pydantic_core-2.14.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=3.10.0.0 (from streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.2->-r requirements.txt (line 6)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.2->-r requirements.txt (line 6)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.2->-r requirements.txt (line 6)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.20.2->-r requirements.txt (line 6)) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit==1.22.0->-r requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit==1.22.0->-r requirements.txt (line 8)) (2.16.1)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio>=3.50->-r requirements.txt (line 12)) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore==1.* (from httpx->gradio>=3.50->-r requirements.txt (line 12))\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio>=3.50->-r requirements.txt (line 12)) (1.3.0)\n",
            "Collecting coloredlogs (from onnxruntime->rembg->-r requirements.txt (line 4))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg->-r requirements.txt (line 4)) (23.5.26)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg->-r requirements.txt (line 4)) (4.1.0)\n",
            "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg->-r requirements.txt (line 4)) (0.58.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg->-r requirements.txt (line 4)) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg->-r requirements.txt (line 4)) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg->-r requirements.txt (line 4)) (1.5.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi->gradio>=3.50->-r requirements.txt (line 12)) (1.2.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.22.0->-r requirements.txt (line 8))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->streamlit==1.22.0->-r requirements.txt (line 8)) (0.1.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg->-r requirements.txt (line 4)) (0.41.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->rembg->-r requirements.txt (line 4))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: diffusers, segment-anything, fire, ffmpy\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.20.2-py3-none-any.whl size=1342632 sha256=d9e8b3bffef660d0f47c3f975517e6d53d115ba3008984769fd4e11f0c912ad4\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/8b/d9/34f7a1936109e05e9bba0cc2241a6f8cd89e25959dc7aae942\n",
            "  Building wheel for segment-anything (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segment-anything: filename=segment_anything-1.0-py3-none-any.whl size=36587 sha256=ee7001ad975326c8d36d717e8bb742d7dc144ff918c1240fdf9000e0e786026e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8ps5s0gj/wheels/10/cf/59/9ccb2f0a1bcc81d4fbd0e501680b5d088d690c6cfbc02dc99d\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=69e3d5828b25f07ae9dc42e4fbf16e52adb9502633c27ac86bfaa5cca029d686\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=7e6e06a149ec2fe5709b0da84759601e5cd15dc334a356b21079decdf5649829\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built diffusers segment-anything fire ffmpy\n",
            "Installing collected packages: tokenizers, segment-anything, pydub, ffmpy, websockets, watchdog, validators, typing-extensions, tomlkit, smmap, shellingham, semantic-version, python-multipart, pympler, orjson, humanfriendly, h11, fire, colorama, annotated-types, aiofiles, uvicorn, starlette, pymatting, pydeck, pydantic-core, httpcore, gitdb, coloredlogs, transformers, pydantic, onnxruntime, httpx, gitpython, diffusers, rembg, gradio-client, fastapi, streamlit, gradio\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 annotated-types-0.6.0 colorama-0.4.6 coloredlogs-15.0.1 diffusers-0.20.2 fastapi-0.105.0 ffmpy-0.3.1 fire-0.5.0 gitdb-4.0.11 gitpython-3.1.40 gradio-4.10.0 gradio-client-0.7.3 h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 humanfriendly-10.0 onnxruntime-1.16.3 orjson-3.9.10 pydantic-2.5.2 pydantic-core-2.14.5 pydeck-0.8.1b0 pydub-0.25.1 pymatting-1.1.12 pympler-1.0.1 python-multipart-0.0.6 rembg-2.0.53 segment-anything-1.0 semantic-version-2.10.0 shellingham-1.5.4 smmap-5.0.1 starlette-0.27.0 streamlit-1.22.0 tokenizers-0.13.3 tomlkit-0.12.0 transformers-4.29.2 typing-extensions-4.9.0 uvicorn-0.24.0.post1 validators-0.22.0 watchdog-3.0.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/SUDO-AI-3D/zero123plus.git\n",
        "%cd /content/zero123plus\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VPKIO2kHFFFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HEYqtXimImm",
        "outputId": "cda2a119-243e-40d6-ad1d-2c620f09b717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m579.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.23.5)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from iopath) (4.5.0)\n",
            "Collecting portalocker (from iopath)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=41f258e875478eb9d3c6c95106c4b771ea512979866b0ebc2e0b07bb66b40630\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=670455cb857722d4817ebd25a3e3bf39f82bbc4cc155196b5254ab15cf8bc879\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, iopath, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 portalocker-2.8.2 yacs-0.1.8\n",
            "Looking in links: https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py310_cu121_pyt210/download.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch3d (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch3d\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "need_pytorch3d=False\n",
        "try:\n",
        "    import pytorch3d\n",
        "except ModuleNotFoundError:\n",
        "    need_pytorch3d=True\n",
        "if need_pytorch3d:\n",
        "    if torch.__version__.startswith(\"2.1.\") and sys.platform.startswith(\"linux\"):\n",
        "        # We try to install PyTorch3D via a released wheel.\n",
        "        pyt_version_str=torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        version_str=\"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            torch.version.cuda.replace(\".\",\"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        !pip install fvcore iopath\n",
        "        !pip install --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\n",
        "    else:\n",
        "        # We try to install PyTorch3D from source.\n",
        "        !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KviPRAZ8_OTb",
        "outputId": "c66117a5-0e1c-4424-a965-dcd4c20dd491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rembg in /usr/local/lib/python3.10/dist-packages (2.0.53)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from rembg) (4.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.23.5)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.10/dist-packages (from rembg) (1.16.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from rembg) (4.8.1.78)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from rembg) (9.4.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.10/dist-packages (from rembg) (1.8.0)\n",
            "Requirement already satisfied: pymatting in /usr/local/lib/python3.10/dist-packages (from rembg) (1.1.12)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from rembg) (0.19.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from rembg) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from rembg) (4.66.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (2023.11.2)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.32.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->rembg) (0.13.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime->rembg) (1.12)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (4.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch->rembg) (2.31.0)\n",
            "Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.10/dist-packages (from pymatting->rembg) (0.58.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (2023.12.9)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->rembg) (1.5.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba!=0.49.0->pymatting->rembg) (0.41.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch->rembg) (2023.11.17)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime->rembg) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime->rembg) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rembg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463,
          "referenced_widgets": [
            "dbb65814e92749df9de562d2b8f5b7b6",
            "0dc82541f8bc475b8a64b9f1941b8ee1",
            "a760e222b18f4c6f80d4fac76e0e6676",
            "5fadaed542444b85be74c6bb39a7e22a",
            "de4b1d618754425881e4a6d457cb75bf",
            "a3a38a8e6a9345efba46b670115c7c52",
            "01ba48840d554ad7a8a78bdc96f6ba5e",
            "826044a9951e44d6b6434ed9ead05e4f",
            "acf4f0a3669c49bfaaf7777e5f675249",
            "b847b88318954e429fd7158dcc15c96d",
            "7a6e8fbc9e584c9789209ef7020e3b55"
          ]
        },
        "id": "U8AOEpchmJyr",
        "outputId": "7bdd957c-647c-4abe-ebcd-20800bf61191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbb65814e92749df9de562d2b8f5b7b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b019d68c0685>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Data structures and functions for rendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_objs_as_meshes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVolumes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mso3_exp_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch3d'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# %matplotlib inline\n",
        "# %matplotlib notebook\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import glob\n",
        "import torch\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython import display\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from diffusers import DiffusionPipeline, EulerAncestralDiscreteScheduler\n",
        "\n",
        "from rembg import remove\n",
        "\n",
        "# Data structures and functions for rendering\n",
        "from pytorch3d.io import load_objs_as_meshes\n",
        "from pytorch3d.structures import Volumes\n",
        "from pytorch3d.transforms import so3_exp_map\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    NDCMultinomialRaysampler,\n",
        "    MonteCarloRaysampler,\n",
        "    EmissionAbsorptionRaymarcher,\n",
        "    ImplicitRenderer,\n",
        "    RayBundle,\n",
        "    ray_bundle_to_ray_points,\n",
        "    BlendParams,\n",
        "    look_at_view_transform,\n",
        "    MeshRasterizer,\n",
        "    MeshRenderer,\n",
        "    PointLights,\n",
        "    RasterizationSettings,\n",
        "    SoftPhongShader,\n",
        "    SoftSilhouetteShader,\n",
        ")\n",
        "\n",
        "# obtain the utilized device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "else:\n",
        "    print(\n",
        "        'Please note that NeRF is a resource-demanding method.'\n",
        "        + ' Running this notebook on CPU will be extremely slow.'\n",
        "        + ' We recommend running the example on a GPU'\n",
        "        + ' with at least 10 GB of memory.'\n",
        "    )\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oZW3DWHOFtJr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C42cyySBSQbf"
      },
      "outputs": [],
      "source": [
        "# Load the pipeline\n",
        "pipeline = DiffusionPipeline.from_pretrained(\n",
        "    \"sudo-ai/zero123plus-v1.1\", custom_pipeline=\"sudo-ai/zero123plus-pipeline\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipeline.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
        "    pipeline.scheduler.config, timestep_spacing='trailing'\n",
        ")\n",
        "pipeline.to('cuda:0')\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kURn9vlGSqT8"
      },
      "outputs": [],
      "source": [
        "cond = Image.open(requests.get(\"https://d.skis.ltd/nrp/sample-data/lysol.png\", stream=True).raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2khTbvH6U4BH"
      },
      "outputs": [],
      "source": [
        "cond"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXXkvwHJU87R"
      },
      "outputs": [],
      "source": [
        "result = pipeline(cond, num_inference_steps=75).images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIJvvKhZVQao"
      },
      "outputs": [],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNu5wopmWhWE"
      },
      "outputs": [],
      "source": [
        "def generate_cow_renders(result, device):\n",
        "\n",
        "    a_result = np.array(remove(result))[..., 3]\n",
        "    a_result[a_result > 0] = 255\n",
        "    silhouette_result = Image.fromarray(a_result)\n",
        "\n",
        "    target_images = np.zeros((6, 128, 128, 3))\n",
        "    silhouette_binary = np.zeros((6, 128, 128))\n",
        "\n",
        "    c = 0\n",
        "    for i in range(3):\n",
        "      for j in range(2):\n",
        "        im_crop = result.crop((j * 320, i * 320, j * 320 + 320, i * 320 + 320))\n",
        "        sil_crop = silhouette_result.crop((j * 320, i * 320, j * 320 + 320, i * 320 + 320))\n",
        "        target_images[c, ...] = np.array(im_crop.resize((128, 128)))\n",
        "        silhouette_binary[c, ...] = np.array(sil_crop.resize((128, 128)))\n",
        "        c += 1\n",
        "\n",
        "    target_images = torch.tensor(target_images / 255, dtype=torch.float32)\n",
        "    silhouette_binary = torch.tensor(silhouette_binary / 255, dtype=torch.float32)\n",
        "\n",
        "    azim = torch.tensor([30, 90, 150, 210, 270, 330], dtype=torch.float32)\n",
        "    elev = torch.tensor([30, -20, 30, -20, 30, -20], dtype=torch.float32)\n",
        "\n",
        "    R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n",
        "    cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n",
        "\n",
        "    return cameras, target_images, silhouette_binary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RC-UvpS6dAF2"
      },
      "outputs": [],
      "source": [
        "target_cameras, target_images, target_silhouettes = generate_cow_renders(result, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-5iAPbO0hjk"
      },
      "outputs": [],
      "source": [
        "plt.imshow(target_images[0].cpu().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVMlRGhb0wyQ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(target_silhouettes[0].cpu().numpy())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxxgoN2N1CaT"
      },
      "outputs": [],
      "source": [
        "# render_size describes the size of both sides of the\n",
        "# rendered images in pixels. Since an advantage of\n",
        "# Neural Radiance Fields are high quality renders\n",
        "# with a significant amount of details, we render\n",
        "# the implicit function at double the size of\n",
        "# target images.\n",
        "render_size = target_images.shape[1] * 2\n",
        "\n",
        "# Our rendered scene is centered around (0,0,0)\n",
        "# and is enclosed inside a bounding box\n",
        "# whose side is roughly equal to 3.0 (world units).\n",
        "volume_extent_world = 3.0\n",
        "\n",
        "# 1) Instantiate the raysamplers.\n",
        "\n",
        "# Here, NDCMultinomialRaysampler generates a rectangular image\n",
        "# grid of rays whose coordinates follow the PyTorch3D\n",
        "# coordinate conventions.\n",
        "raysampler_grid = NDCMultinomialRaysampler(\n",
        "    image_height=render_size,\n",
        "    image_width=render_size,\n",
        "    n_pts_per_ray=128,\n",
        "    min_depth=0.1,\n",
        "    max_depth=volume_extent_world,\n",
        ")\n",
        "\n",
        "# MonteCarloRaysampler generates a random subset\n",
        "# of `n_rays_per_image` rays emitted from the image plane.\n",
        "raysampler_mc = MonteCarloRaysampler(\n",
        "    min_x = -1.0,\n",
        "    max_x = 1.0,\n",
        "    min_y = -1.0,\n",
        "    max_y = 1.0,\n",
        "    n_rays_per_image=750,\n",
        "    n_pts_per_ray=128,\n",
        "    min_depth=0.1,\n",
        "    max_depth=volume_extent_world,\n",
        ")\n",
        "\n",
        "# 2) Instantiate the raymarcher.\n",
        "# Here, we use the standard EmissionAbsorptionRaymarcher\n",
        "# which marches along each ray in order to render\n",
        "# the ray into a single 3D color vector\n",
        "# and an opacity scalar.\n",
        "raymarcher = EmissionAbsorptionRaymarcher()\n",
        "\n",
        "# Finally, instantiate the implicit renders\n",
        "# for both raysamplers.\n",
        "renderer_grid = ImplicitRenderer(\n",
        "    raysampler=raysampler_grid, raymarcher=raymarcher,\n",
        ")\n",
        "renderer_mc = ImplicitRenderer(\n",
        "    raysampler=raysampler_mc, raymarcher=raymarcher,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_Si0zW1TnZ"
      },
      "outputs": [],
      "source": [
        "class HarmonicEmbedding(torch.nn.Module):\n",
        "    def __init__(self, n_harmonic_functions=60, omega0=0.1):\n",
        "        \"\"\"\n",
        "        Given an input tensor `x` of shape [minibatch, ... , dim],\n",
        "        the harmonic embedding layer converts each feature\n",
        "        in `x` into a series of harmonic features `embedding`\n",
        "        as follows:\n",
        "            embedding[..., i*dim:(i+1)*dim] = [\n",
        "                sin(x[..., i]),\n",
        "                sin(2*x[..., i]),\n",
        "                sin(4*x[..., i]),\n",
        "                ...\n",
        "                sin(2**(self.n_harmonic_functions-1) * x[..., i]),\n",
        "                cos(x[..., i]),\n",
        "                cos(2*x[..., i]),\n",
        "                cos(4*x[..., i]),\n",
        "                ...\n",
        "                cos(2**(self.n_harmonic_functions-1) * x[..., i])\n",
        "            ]\n",
        "\n",
        "        Note that `x` is also premultiplied by `omega0` before\n",
        "        evaluating the harmonic functions.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.register_buffer(\n",
        "            'frequencies',\n",
        "            omega0 * (2.0 ** torch.arange(n_harmonic_functions)),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: tensor of shape [..., dim]\n",
        "        Returns:\n",
        "            embedding: a harmonic embedding of `x`\n",
        "                of shape [..., n_harmonic_functions * dim * 2]\n",
        "        \"\"\"\n",
        "        embed = (x[..., None] * self.frequencies).view(*x.shape[:-1], -1)\n",
        "        return torch.cat((embed.sin(), embed.cos()), dim=-1)\n",
        "\n",
        "\n",
        "class NeuralRadianceField(torch.nn.Module):\n",
        "    def __init__(self, n_harmonic_functions=60, n_hidden_neurons=256):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n_harmonic_functions: The number of harmonic functions\n",
        "                used to form the harmonic embedding of each point.\n",
        "            n_hidden_neurons: The number of hidden units in the\n",
        "                fully connected layers of the MLPs of the model.\n",
        "        \"\"\"\n",
        "\n",
        "        # The harmonic embedding layer converts input 3D coordinates\n",
        "        # to a representation that is more suitable for\n",
        "        # processing with a deep neural network.\n",
        "        self.harmonic_embedding = HarmonicEmbedding(n_harmonic_functions)\n",
        "\n",
        "        # The dimension of the harmonic embedding.\n",
        "        embedding_dim = n_harmonic_functions * 2 * 3\n",
        "\n",
        "        # self.mlp is a simple 2-layer multi-layer perceptron\n",
        "        # which converts the input per-point harmonic embeddings\n",
        "        # to a latent representation.\n",
        "        # Not that we use Softplus activations instead of ReLU.\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(embedding_dim, n_hidden_neurons),\n",
        "            torch.nn.Softplus(beta=10.0),\n",
        "            torch.nn.Linear(n_hidden_neurons, n_hidden_neurons),\n",
        "            torch.nn.Softplus(beta=10.0),\n",
        "        )\n",
        "\n",
        "        # Given features predicted by self.mlp, self.color_layer\n",
        "        # is responsible for predicting a 3-D per-point vector\n",
        "        # that represents the RGB color of the point.\n",
        "        self.color_layer = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_hidden_neurons + embedding_dim, n_hidden_neurons),\n",
        "            torch.nn.Softplus(beta=10.0),\n",
        "            torch.nn.Linear(n_hidden_neurons, 3),\n",
        "            torch.nn.Sigmoid(),\n",
        "            # To ensure that the colors correctly range between [0-1],\n",
        "            # the layer is terminated with a sigmoid layer.\n",
        "        )\n",
        "\n",
        "        # The density layer converts the features of self.mlp\n",
        "        # to a 1D density value representing the raw opacity\n",
        "        # of each point.\n",
        "        self.density_layer = torch.nn.Sequential(\n",
        "            torch.nn.Linear(n_hidden_neurons, 1),\n",
        "            torch.nn.Softplus(beta=10.0),\n",
        "            # Sofplus activation ensures that the raw opacity\n",
        "            # is a non-negative number.\n",
        "        )\n",
        "\n",
        "        # We set the bias of the density layer to -1.5\n",
        "        # in order to initialize the opacities of the\n",
        "        # ray points to values close to 0.\n",
        "        # This is a crucial detail for ensuring convergence\n",
        "        # of the model.\n",
        "        self.density_layer[0].bias.data[0] = -1.5\n",
        "\n",
        "    def _get_densities(self, features):\n",
        "        \"\"\"\n",
        "        This function takes `features` predicted by `self.mlp`\n",
        "        and converts them to `raw_densities` with `self.density_layer`.\n",
        "        `raw_densities` are later mapped to [0-1] range with\n",
        "        1 - inverse exponential of `raw_densities`.\n",
        "        \"\"\"\n",
        "        raw_densities = self.density_layer(features)\n",
        "        return 1 - (-raw_densities).exp()\n",
        "\n",
        "    def _get_colors(self, features, rays_directions):\n",
        "        \"\"\"\n",
        "        This function takes per-point `features` predicted by `self.mlp`\n",
        "        and evaluates the color model in order to attach to each\n",
        "        point a 3D vector of its RGB color.\n",
        "\n",
        "        In order to represent viewpoint dependent effects,\n",
        "        before evaluating `self.color_layer`, `NeuralRadianceField`\n",
        "        concatenates to the `features` a harmonic embedding\n",
        "        of `ray_directions`, which are per-point directions\n",
        "        of point rays expressed as 3D l2-normalized vectors\n",
        "        in world coordinates.\n",
        "        \"\"\"\n",
        "        spatial_size = features.shape[:-1]\n",
        "\n",
        "        # Normalize the ray_directions to unit l2 norm.\n",
        "        rays_directions_normed = torch.nn.functional.normalize(\n",
        "            rays_directions, dim=-1\n",
        "        )\n",
        "\n",
        "        # Obtain the harmonic embedding of the normalized ray directions.\n",
        "        rays_embedding = self.harmonic_embedding(\n",
        "            rays_directions_normed\n",
        "        )\n",
        "\n",
        "        # Expand the ray directions tensor so that its spatial size\n",
        "        # is equal to the size of features.\n",
        "        rays_embedding_expand = rays_embedding[..., None, :].expand(\n",
        "            *spatial_size, rays_embedding.shape[-1]\n",
        "        )\n",
        "\n",
        "        # Concatenate ray direction embeddings with\n",
        "        # features and evaluate the color model.\n",
        "        color_layer_input = torch.cat(\n",
        "            (features, rays_embedding_expand),\n",
        "            dim=-1\n",
        "        )\n",
        "        return self.color_layer(color_layer_input)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        ray_bundle: RayBundle,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        The forward function accepts the parametrizations of\n",
        "        3D points sampled along projection rays. The forward\n",
        "        pass is responsible for attaching a 3D vector\n",
        "        and a 1D scalar representing the point's\n",
        "        RGB color and opacity respectively.\n",
        "\n",
        "        Args:\n",
        "            ray_bundle: A RayBundle object containing the following variables:\n",
        "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
        "                    origins of the sampling rays in world coords.\n",
        "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
        "                    containing the direction vectors of sampling rays in world coords.\n",
        "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
        "                    containing the lengths at which the rays are sampled.\n",
        "\n",
        "        Returns:\n",
        "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
        "                denoting the opacity of each ray point.\n",
        "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
        "                denoting the color of each ray point.\n",
        "        \"\"\"\n",
        "        # We first convert the ray parametrizations to world\n",
        "        # coordinates with `ray_bundle_to_ray_points`.\n",
        "        rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
        "        # rays_points_world.shape = [minibatch x ... x 3]\n",
        "\n",
        "        # For each 3D world coordinate, we obtain its harmonic embedding.\n",
        "        embeds = self.harmonic_embedding(\n",
        "            rays_points_world\n",
        "        )\n",
        "        # embeds.shape = [minibatch x ... x self.n_harmonic_functions*6]\n",
        "\n",
        "        # self.mlp maps each harmonic embedding to a latent feature space.\n",
        "        features = self.mlp(embeds)\n",
        "        # features.shape = [minibatch x ... x n_hidden_neurons]\n",
        "\n",
        "        # Finally, given the per-point features,\n",
        "        # execute the density and color branches.\n",
        "\n",
        "        rays_densities = self._get_densities(features)\n",
        "        # rays_densities.shape = [minibatch x ... x 1]\n",
        "\n",
        "        rays_colors = self._get_colors(features, ray_bundle.directions)\n",
        "        # rays_colors.shape = [minibatch x ... x 3]\n",
        "\n",
        "        return rays_densities, rays_colors\n",
        "\n",
        "    def batched_forward(\n",
        "        self,\n",
        "        ray_bundle: RayBundle,\n",
        "        n_batches: int = 16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This function is used to allow for memory efficient processing\n",
        "        of input rays. The input rays are first split to `n_batches`\n",
        "        chunks and passed through the `self.forward` function one at a time\n",
        "        in a for loop. Combined with disabling PyTorch gradient caching\n",
        "        (`torch.no_grad()`), this allows for rendering large batches\n",
        "        of rays that do not all fit into GPU memory in a single forward pass.\n",
        "        In our case, batched_forward is used to export a fully-sized render\n",
        "        of the radiance field for visualization purposes.\n",
        "\n",
        "        Args:\n",
        "            ray_bundle: A RayBundle object containing the following variables:\n",
        "                origins: A tensor of shape `(minibatch, ..., 3)` denoting the\n",
        "                    origins of the sampling rays in world coords.\n",
        "                directions: A tensor of shape `(minibatch, ..., 3)`\n",
        "                    containing the direction vectors of sampling rays in world coords.\n",
        "                lengths: A tensor of shape `(minibatch, ..., num_points_per_ray)`\n",
        "                    containing the lengths at which the rays are sampled.\n",
        "            n_batches: Specifies the number of batches the input rays are split into.\n",
        "                The larger the number of batches, the smaller the memory footprint\n",
        "                and the lower the processing speed.\n",
        "\n",
        "        Returns:\n",
        "            rays_densities: A tensor of shape `(minibatch, ..., num_points_per_ray, 1)`\n",
        "                denoting the opacity of each ray point.\n",
        "            rays_colors: A tensor of shape `(minibatch, ..., num_points_per_ray, 3)`\n",
        "                denoting the color of each ray point.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Parse out shapes needed for tensor reshaping in this function.\n",
        "        n_pts_per_ray = ray_bundle.lengths.shape[-1]\n",
        "        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]\n",
        "\n",
        "        # Split the rays to `n_batches` batches.\n",
        "        tot_samples = ray_bundle.origins.shape[:-1].numel()\n",
        "        batches = torch.chunk(torch.arange(tot_samples), n_batches)\n",
        "\n",
        "        # For each batch, execute the standard forward pass.\n",
        "        batch_outputs = [\n",
        "            self.forward(\n",
        "                RayBundle(\n",
        "                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],\n",
        "                    directions=ray_bundle.directions.view(-1, 3)[batch_idx],\n",
        "                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],\n",
        "                    xys=None,\n",
        "                )\n",
        "            ) for batch_idx in batches\n",
        "        ]\n",
        "\n",
        "        # Concatenate the per-batch rays_densities and rays_colors\n",
        "        # and reshape according to the sizes of the inputs.\n",
        "        rays_densities, rays_colors = [\n",
        "            torch.cat(\n",
        "                [batch_output[output_i] for batch_output in batch_outputs], dim=0\n",
        "            ).view(*spatial_size, -1) for output_i in (0, 1)\n",
        "        ]\n",
        "        return rays_densities, rays_colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB4vToGI1Z-R"
      },
      "outputs": [],
      "source": [
        "def huber(x, y, scaling=0.1):\n",
        "    \"\"\"\n",
        "    A helper function for evaluating the smooth L1 (huber) loss\n",
        "    between the rendered silhouettes and colors.\n",
        "    \"\"\"\n",
        "    diff_sq = (x - y) ** 2\n",
        "    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n",
        "    return loss\n",
        "\n",
        "def sample_images_at_mc_locs(target_images, sampled_rays_xy):\n",
        "    \"\"\"\n",
        "    Given a set of Monte Carlo pixel locations `sampled_rays_xy`,\n",
        "    this method samples the tensor `target_images` at the\n",
        "    respective 2D locations.\n",
        "\n",
        "    This function is used in order to extract the colors from\n",
        "    ground truth images that correspond to the colors\n",
        "    rendered using `MonteCarloRaysampler`.\n",
        "    \"\"\"\n",
        "    ba = target_images.shape[0]\n",
        "    dim = target_images.shape[-1]\n",
        "    spatial_size = sampled_rays_xy.shape[1:-1]\n",
        "    # In order to sample target_images, we utilize\n",
        "    # the grid_sample function which implements a\n",
        "    # bilinear image sampler.\n",
        "    # Note that we have to invert the sign of the\n",
        "    # sampled ray positions to convert the NDC xy locations\n",
        "    # of the MonteCarloRaysampler to the coordinate\n",
        "    # convention of grid_sample.\n",
        "    images_sampled = torch.nn.functional.grid_sample(\n",
        "        target_images.permute(0, 3, 1, 2),\n",
        "        -sampled_rays_xy.view(ba, -1, 1, 2),  # note the sign inversion\n",
        "        align_corners=True\n",
        "    )\n",
        "    return images_sampled.permute(0, 2, 3, 1).view(\n",
        "        ba, *spatial_size, dim\n",
        "    )\n",
        "\n",
        "def show_full_render(\n",
        "    neural_radiance_field, camera,\n",
        "    target_image, target_silhouette,\n",
        "    loss_history_color, loss_history_sil,\n",
        "):\n",
        "    \"\"\"\n",
        "    This is a helper function for visualizing the\n",
        "    intermediate results of the learning.\n",
        "\n",
        "    Since the `NeuralRadianceField` suffers from\n",
        "    a large memory footprint, which does not let us\n",
        "    render the full image grid in a single forward pass,\n",
        "    we utilize the `NeuralRadianceField.batched_forward`\n",
        "    function in combination with disabling the gradient caching.\n",
        "    This chunks the set of emitted rays to batches and\n",
        "    evaluates the implicit function on one batch at a time\n",
        "    to prevent GPU memory overflow.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prevent gradient caching.\n",
        "    with torch.no_grad():\n",
        "        # Render using the grid renderer and the\n",
        "        # batched_forward function of neural_radiance_field.\n",
        "        rendered_image_silhouette, _ = renderer_grid(\n",
        "            cameras=camera,\n",
        "            volumetric_function=neural_radiance_field.batched_forward\n",
        "        )\n",
        "        # Split the rendering result to a silhouette render\n",
        "        # and the image render.\n",
        "        rendered_image, rendered_silhouette = (\n",
        "            rendered_image_silhouette[0].split([3, 1], dim=-1)\n",
        "        )\n",
        "\n",
        "    # Generate plots.\n",
        "    fig, ax = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    ax = ax.ravel()\n",
        "    clamp_and_detach = lambda x: x.clamp(0.0, 1.0).cpu().detach().numpy()\n",
        "    ax[0].plot(list(range(len(loss_history_color))), loss_history_color, linewidth=1)\n",
        "    ax[1].imshow(clamp_and_detach(rendered_image))\n",
        "    ax[2].imshow(clamp_and_detach(rendered_silhouette[..., 0]))\n",
        "    ax[3].plot(list(range(len(loss_history_sil))), loss_history_sil, linewidth=1)\n",
        "    ax[4].imshow(clamp_and_detach(target_image))\n",
        "    ax[5].imshow(clamp_and_detach(target_silhouette))\n",
        "    for ax_, title_ in zip(\n",
        "        ax,\n",
        "        (\n",
        "            \"loss color\", \"rendered image\", \"rendered silhouette\",\n",
        "            \"loss silhouette\", \"target image\",  \"target silhouette\",\n",
        "        )\n",
        "    ):\n",
        "        if not title_.startswith('loss'):\n",
        "            ax_.grid(\"off\")\n",
        "            ax_.axis(\"off\")\n",
        "        ax_.set_title(title_)\n",
        "    fig.canvas.draw(); fig.show()\n",
        "    display.clear_output(wait=True)\n",
        "    display.display(fig)\n",
        "    return fig\n",
        "\n",
        "def image_grid(\n",
        "    images,\n",
        "    rows=None,\n",
        "    cols=None,\n",
        "    fill: bool = True,\n",
        "    show_axes: bool = False,\n",
        "    rgb: bool = True,\n",
        "    save_gif = False\n",
        "):\n",
        "    \"\"\"\n",
        "    A util function for plotting a grid of images.\n",
        "\n",
        "    Args:\n",
        "        images: (N, H, W, 4) array of RGBA images\n",
        "        rows: number of rows in the grid\n",
        "        cols: number of columns in the grid\n",
        "        fill: boolean indicating if the space between images should be filled\n",
        "        show_axes: boolean indicating if the axes of the plots should be visible\n",
        "        rgb: boolean, If True, only RGB channels are plotted.\n",
        "            If False, only the alpha channel is plotted.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    if (rows is None) != (cols is None):\n",
        "        raise ValueError(\"Specify either both rows and cols or neither.\")\n",
        "\n",
        "    if rows is None:\n",
        "        rows = len(images)\n",
        "        cols = 1\n",
        "\n",
        "    gridspec_kw = {\"wspace\": 0.0, \"hspace\": 0.0} if fill else {}\n",
        "    fig, axarr = plt.subplots(rows, cols, gridspec_kw=gridspec_kw, figsize=(15, 9))\n",
        "    bleed = 0\n",
        "    fig.subplots_adjust(left=bleed, bottom=bleed, right=(1 - bleed), top=(1 - bleed))\n",
        "\n",
        "    for ax, im in zip(axarr.ravel(), images):\n",
        "        if rgb:\n",
        "            # only render RGB channels\n",
        "            ax.imshow(im[..., :3])\n",
        "        else:\n",
        "            # only render Alpha channel\n",
        "            ax.imshow(im[..., 3])\n",
        "        if not show_axes:\n",
        "            ax.set_axis_off()\n",
        "\n",
        "    if save_gif:\n",
        "        frames = [Image.fromarray((img[..., :3]*255).astype(np.uint8)) for img in images]\n",
        "        frame_one = frames[0]\n",
        "        frame_one.save(\"/content/3DR.gif\", format=\"GIF\", append_images=frames,\n",
        "               save_all=True, duration=100, loop=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCyGpeHu1lO8"
      },
      "outputs": [],
      "source": [
        "# First move all relevant variables to the correct device.\n",
        "renderer_grid = renderer_grid.to(device)\n",
        "renderer_mc = renderer_mc.to(device)\n",
        "target_cameras = target_cameras.to(device)\n",
        "target_images = target_images.to(device)\n",
        "target_silhouettes = target_silhouettes.to(device)\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Instantiate the radiance field model.\n",
        "neural_radiance_field = NeuralRadianceField().to(device)\n",
        "\n",
        "# Instantiate the Adam optimizer. We set its master learning rate to 1e-3.\n",
        "lr = 1e-3\n",
        "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr)\n",
        "\n",
        "# We sample 6 random cameras in a minibatch. Each camera\n",
        "# emits raysampler_mc.n_pts_per_image rays.\n",
        "batch_size = 6\n",
        "\n",
        "# 3000 iterations take ~20 min on a Tesla M40 and lead to\n",
        "# reasonably sharp results. However, for the best possible\n",
        "# results, we recommend setting n_iter=20000.\n",
        "n_iter = 3000\n",
        "\n",
        "# Init the loss history buffers.\n",
        "loss_history_color, loss_history_sil = [], []\n",
        "\n",
        "# The main optimization loop.\n",
        "for iteration in range(n_iter):\n",
        "    # In case we reached the last 75% of iterations,\n",
        "    # decrease the learning rate of the optimizer 10-fold.\n",
        "    if iteration == round(n_iter * 0.75):\n",
        "        print('Decreasing LR 10-fold ...')\n",
        "        optimizer = torch.optim.Adam(\n",
        "            neural_radiance_field.parameters(), lr=lr * 0.1\n",
        "        )\n",
        "\n",
        "    # Zero the optimizer gradient.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Sample random batch indices.\n",
        "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
        "\n",
        "    # Sample the minibatch of cameras.\n",
        "    batch_cameras = FoVPerspectiveCameras(\n",
        "        R = target_cameras.R[batch_idx],\n",
        "        T = target_cameras.T[batch_idx],\n",
        "        znear = target_cameras.znear[batch_idx],\n",
        "        zfar = target_cameras.zfar[batch_idx],\n",
        "        aspect_ratio = target_cameras.aspect_ratio[batch_idx],\n",
        "        fov = target_cameras.fov[batch_idx],\n",
        "        device = device,\n",
        "    )\n",
        "\n",
        "    # Evaluate the nerf model.\n",
        "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
        "        cameras=batch_cameras,\n",
        "        volumetric_function=neural_radiance_field\n",
        "    )\n",
        "    rendered_images, rendered_silhouettes = (\n",
        "        rendered_images_silhouettes.split([3, 1], dim=-1)\n",
        "    )\n",
        "\n",
        "    # Compute the silhouette error as the mean huber\n",
        "    # loss between the predicted masks and the\n",
        "    # sampled target silhouettes.\n",
        "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
        "        target_silhouettes[batch_idx, ..., None],\n",
        "        sampled_rays.xys\n",
        "    )\n",
        "    sil_err = huber(\n",
        "        rendered_silhouettes,\n",
        "        silhouettes_at_rays,\n",
        "    ).abs().mean()\n",
        "\n",
        "    # Compute the color error as the mean huber\n",
        "    # loss between the rendered colors and the\n",
        "    # sampled target images.\n",
        "    colors_at_rays = sample_images_at_mc_locs(\n",
        "        target_images[batch_idx],\n",
        "        sampled_rays.xys\n",
        "    )\n",
        "    color_err = huber(\n",
        "        rendered_images,\n",
        "        colors_at_rays,\n",
        "    ).abs().mean()\n",
        "\n",
        "    # The optimization loss is a simple\n",
        "    # sum of the color and silhouette errors.\n",
        "    loss = color_err + sil_err\n",
        "\n",
        "    # Log the loss history.\n",
        "    loss_history_color.append(float(color_err))\n",
        "    loss_history_sil.append(float(sil_err))\n",
        "\n",
        "    # Every 10 iterations, print the current values of the losses.\n",
        "    if iteration % 10 == 0:\n",
        "        print(\n",
        "            f'Iteration {iteration:05d}:'\n",
        "            + f' loss color = {float(color_err):1.2e}'\n",
        "            + f' loss silhouette = {float(sil_err):1.2e}'\n",
        "        )\n",
        "\n",
        "    # Take the optimization step.\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Visualize the full renders every 100 iterations.\n",
        "    if iteration % 100 == 0:\n",
        "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
        "        show_full_render(\n",
        "            neural_radiance_field,\n",
        "            FoVPerspectiveCameras(\n",
        "                R = target_cameras.R[show_idx],\n",
        "                T = target_cameras.T[show_idx],\n",
        "                znear = target_cameras.znear[show_idx],\n",
        "                zfar = target_cameras.zfar[show_idx],\n",
        "                aspect_ratio = target_cameras.aspect_ratio[show_idx],\n",
        "                fov = target_cameras.fov[show_idx],\n",
        "                device = device,\n",
        "            ),\n",
        "            target_images[show_idx][0],\n",
        "            target_silhouettes[show_idx][0],\n",
        "            loss_history_color,\n",
        "            loss_history_sil,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CMd3E2l1-O9"
      },
      "outputs": [],
      "source": [
        "def generate_rotating_nerf(neural_radiance_field, n_frames = 50):\n",
        "    logRs = torch.zeros(n_frames, 3, device=device)\n",
        "    logRs[:, 1] = torch.linspace(-3.14, 3.14, n_frames, device=device)\n",
        "    Rs = so3_exp_map(logRs)\n",
        "    Ts = torch.zeros(n_frames, 3, device=device)\n",
        "    Ts[:, 2] = 2.7\n",
        "    frames = []\n",
        "    print('Rendering rotating NeRF ...')\n",
        "    for R, T in zip(tqdm(Rs), Ts):\n",
        "        camera = FoVPerspectiveCameras(\n",
        "            R=R[None],\n",
        "            T=T[None],\n",
        "            znear=target_cameras.znear[0],\n",
        "            zfar=target_cameras.zfar[0],\n",
        "            aspect_ratio=target_cameras.aspect_ratio[0],\n",
        "            fov=target_cameras.fov[0],\n",
        "            device=device,\n",
        "        )\n",
        "        # Note that we again render with `NDCMultinomialRaysampler`\n",
        "        # and the batched_forward function of neural_radiance_field.\n",
        "        frames.append(\n",
        "            renderer_grid(\n",
        "                cameras=camera,\n",
        "                volumetric_function=neural_radiance_field.batched_forward,\n",
        "            )[0][..., :3]\n",
        "        )\n",
        "    return torch.cat(frames)\n",
        "\n",
        "with torch.no_grad():\n",
        "    rotating_nerf_frames = generate_rotating_nerf(neural_radiance_field, n_frames=3*5)\n",
        "\n",
        "image_grid(rotating_nerf_frames.clamp(0., 1.).cpu().numpy(), rows=3, cols=5, rgb=True, fill=True, save_gif=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "Image(open('/content/3DR.gif', 'rb').read())  # local"
      ],
      "metadata": {
        "id": "eT1tdMfgrkll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YIaWNwxPO1sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7VcHjO-kO2To"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Yeth8Y5_vD9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dbb65814e92749df9de562d2b8f5b7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0dc82541f8bc475b8a64b9f1941b8ee1",
              "IPY_MODEL_a760e222b18f4c6f80d4fac76e0e6676",
              "IPY_MODEL_5fadaed542444b85be74c6bb39a7e22a"
            ],
            "layout": "IPY_MODEL_de4b1d618754425881e4a6d457cb75bf"
          }
        },
        "0dc82541f8bc475b8a64b9f1941b8ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3a38a8e6a9345efba46b670115c7c52",
            "placeholder": "​",
            "style": "IPY_MODEL_01ba48840d554ad7a8a78bdc96f6ba5e",
            "value": ""
          }
        },
        "a760e222b18f4c6f80d4fac76e0e6676": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_826044a9951e44d6b6434ed9ead05e4f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acf4f0a3669c49bfaaf7777e5f675249",
            "value": 0
          }
        },
        "5fadaed542444b85be74c6bb39a7e22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b847b88318954e429fd7158dcc15c96d",
            "placeholder": "​",
            "style": "IPY_MODEL_7a6e8fbc9e584c9789209ef7020e3b55",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "de4b1d618754425881e4a6d457cb75bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a38a8e6a9345efba46b670115c7c52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ba48840d554ad7a8a78bdc96f6ba5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "826044a9951e44d6b6434ed9ead05e4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "acf4f0a3669c49bfaaf7777e5f675249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b847b88318954e429fd7158dcc15c96d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a6e8fbc9e584c9789209ef7020e3b55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}